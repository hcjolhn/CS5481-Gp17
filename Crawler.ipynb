{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install requests\n",
    "! pip install BeautifulSoup4\n",
    "! pip install nltk\n",
    "import requests as r\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def fetch_bbc():\n",
    "    start_date = datetime.date(2015,7,1)\n",
    "    end_date = datetime.date(2023,11,9)\n",
    "    current_date = start_date\n",
    "    sleep=1\n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        day = current_date.day\n",
    "        print(f\"Year: {year}, Month: {month}, Day: {day}\")\n",
    "        url = f\"https://dracos.co.uk/made/bbc-news-archive/{year}/{month:0>2}/{day:0>2}/\"\n",
    "        res, error = retry(url)\n",
    "        if not error:\n",
    "            html = res.text\n",
    "            bf = BeautifulSoup(html)\n",
    "            url_set = set([x['href'] for x in bf.find_all('a', href=True) if re.search(r\"\\d{8}\", x['href'])])\n",
    "            urls = enumerate(url_set)\n",
    "            with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "                {executor.submit(crawler,url,'bbc',idx, f\"{year}{month:0>2}{day:0>2}\"):(idx, url) for idx, url in urls}\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "\n",
    "def fetch_nbc():\n",
    "    months = [\n",
    "    'january', 'february', 'march', 'april', 'may', 'june', \n",
    "    'july', 'august', 'september', 'october', 'november', 'december'\n",
    "    ]\n",
    "    for year in range(2003,2024):\n",
    "        for month in months:\n",
    "            print(f\"Year: {year}, Month: {month}\")\n",
    "            done = False\n",
    "            pageNo = 1\n",
    "            page_list = []\n",
    "            while not done:\n",
    "                url = f\"https://www.nbcnews.com/archive/articles/{year}/{month}/\"\n",
    "                if pageNo != 1:\n",
    "                    url = f\"https://www.nbcnews.com/archive/articles/{year}/{month}/{pageNo}/\"\n",
    "                res, error = retry(url)\n",
    "                if not error:\n",
    "                    html = res.text\n",
    "                    bf = BeautifulSoup(html)\n",
    "                    if pageNo == 1:\n",
    "                        page_list = [x.text for x in bf.find_all('a', class_='Pagination__num Pagination__enable')]\n",
    "                    url_set = set([x['href'] for x in bf.main.find_all('a', href=True)])\n",
    "                    urls = enumerate(url_set)\n",
    "                    month_no = months.index(month)+1\n",
    "                    with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "                        {executor.submit(crawler,url,'nbc',idx, f\"{year}{month_no:0>2}{pageNo}\"):(idx, url) for idx, url in urls}\n",
    "                if len(page_list) == 0 or pageNo == int(page_list[-1]) or error: \n",
    "                    done = True\n",
    "                else:\n",
    "                    pageNo = int(page_list[pageNo-1])\n",
    "                    \n",
    "def fetch_cnn():\n",
    "    for year in range(2011,2024):\n",
    "        url = f\"https://edition.cnn.com/article/sitemap-{year}.html\"\n",
    "        res, error = retry(url)\n",
    "        if not error:\n",
    "            html = res.text\n",
    "            bf = BeautifulSoup(html)\n",
    "            url_set = set([x['href'] for x in bf.body.find_all('section')[0].find_all('a', href=True)])\n",
    "            print(url_set)\n",
    "            for url in url_set:\n",
    "                url = f\"https://edition.cnn.com{url}\"\n",
    "                res, error = retry(url)\n",
    "                if not error:\n",
    "                    html = res.text\n",
    "                    bf = BeautifulSoup(html)\n",
    "                    url_list = [x['href'] for x in bf.body.find_all('div', class_='sitemap-entries')[0].find_all('a', href=True)]\n",
    "                    date_list = [x.text for x in bf.body.find_all('div', class_='sitemap-entries')[0].find_all('span', class_='date')][1:]\n",
    "                    with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "                        {executor.submit(crawler,url,'cnn',idx, date_list[idx]):(idx, url) for idx, url in enumerate(url_list)}\n",
    "\n",
    "def fetch_dailymail():\n",
    "    for year in range(1994,2024):\n",
    "        url = f\"https://www.dailymail.co.uk/home/sitemaparchive/year_{year}.html\"\n",
    "        res, error = retry(url)\n",
    "        if not error:\n",
    "            html = res.text\n",
    "            bf = BeautifulSoup(html)\n",
    "            url_set = set([x['href'] for x in bf.body.find_all('ul', class_='archive-index home link-box')[0].find_all('a', href=True)])\n",
    "            for url_ in url_set:\n",
    "                url = f\"https://www.dailymail.co.uk{url_}\"\n",
    "                res, error = retry(url)\n",
    "                if not error:\n",
    "                    html = res.text\n",
    "                    bf = BeautifulSoup(html)\n",
    "                    url_list = [x['href'] for x in bf.body.find_all('ul', class_='archive-articles debate link-box')[0].find_all('a', href=True)]\n",
    "                    date = url_.split(\".\")[0][-8:]\n",
    "                    with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "                        {executor.submit(crawler,f\"https://www.dailymail.co.uk{url}\",'dailymail',idx, date):(idx, url) for idx, url in enumerate(url_list)}\n",
    "\n",
    "                \n",
    "def crawler(url,source,idx,date):\n",
    "    #print(url,source,idx,date)\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    try:\n",
    "        res = r.get(url,headers)\n",
    "        time.sleep(3)\n",
    "        if(res.status_code == 200):\n",
    "            html = res.text\n",
    "            bf = BeautifulSoup(html)\n",
    "            data = {}\n",
    "            if source == \"bbc\":\n",
    "                os.makedirs(\"crawl/bbc\", exist_ok=True)\n",
    "                file_path = \"crawl/bbc/\"+f\"{date}{idx:0>3}\"+\".json\"\n",
    "                data['title'] = \" \".join(bf.h1.text.split())\n",
    "                data['url'] = url\n",
    "                data['date'] = bf.find('time')['datetime'][:10]\n",
    "                data['keywords'] = keywords_generator(re.sub(r\"[^\\w\\s]+\",\"\",\" \".join(bf.find('article').text.split()).lower()))\n",
    "                data['category'] = url[8:].split(\"/\")[1]\n",
    "                if url[8:].split(\"/\")[1] == \"news\":\n",
    "                    pattern = r\"news/(.*?\\d{8})\"\n",
    "                    match = re.search(pattern, url)\n",
    "                    if match:\n",
    "                        array = match.group(0).split('/')\n",
    "                        last_item = array[-1]\n",
    "                        if re.match(r\".*-\\d{8}$\", last_item):\n",
    "                            array[-1:] = last_item.rsplit('-', 1)\n",
    "                        data['category'] = array[:-1]\n",
    "                if url[8:].split(\"/\")[1] == \"sport\":\n",
    "                    pattern = r\"sport/(.*?)/\\d{8}\"\n",
    "                    match = re.search(pattern, url)\n",
    "                    if match:\n",
    "                        data['category'] = match.group(0).split('/')[:-1]\n",
    "                json_object = json.dumps(data, indent=4)\n",
    "                with open(file_path, \"w\") as outfile:\n",
    "                    outfile.write(json_object)\n",
    "            if source == \"nbc\":\n",
    "                os.makedirs(\"crawl/nbc\", exist_ok=True)\n",
    "                file_path = \"crawl/nbc/\"+f\"{date}{idx:0>4}\"+\".json\"\n",
    "                data['title'] = \" \".join(bf.h1.text.split())\n",
    "                data['url'] = url\n",
    "                data['date'] = bf.find('time')['datetime'][:10]\n",
    "                data['keywords'] = keywords_generator(re.sub(r\"[^\\w\\s]+\",\"\",\" \".join(bf.find('article').text.split()).lower()))\n",
    "                data['category'] = [bf.find_all('span', attrs={'data-testid': 'unibrow-text'})[0].text.lower()]\n",
    "                json_object = json.dumps(data, indent=4)\n",
    "                with open(file_path, \"w\") as outfile:\n",
    "                    outfile.write(json_object)\n",
    "            if source == \"cnn\":\n",
    "                os.makedirs(\"crawl/cnn\", exist_ok=True)\n",
    "                file_path = \"crawl/cnn/\"+date.replace(\"-\",\"\")+f\"{idx:0>4}\"+\".json\"\n",
    "                data['title'] = \" \".join(bf.h1.text.split())\n",
    "                data['url'] = url\n",
    "                pattern = r'[A-Za-z]+\\s([A-Za-z]+)\\s(\\d+),\\s(\\d{4})'\n",
    "                match = re.search(pattern, \" \".join(bf.find('div', class_='timestamp').text.split()))\n",
    "                data['date'] = f\"{int(match.group(3))}-{time.strptime(match.group(1), '%B').tm_mon:02d}-{int(match.group(2)):02d}\"\n",
    "                data['keywords'] = keywords_generator(re.sub(r\"[^\\w\\s]+\",\"\",\" \".join(bf.find('div', class_='article__content-container').text.split()).lower()))\n",
    "                category = url.split(\"/\")\n",
    "                data['category'] = [category[6]]\n",
    "                if \"-\" not in category[7]:\n",
    "                   data['category'] = [category[6],category[7]]\n",
    "                json_object = json.dumps(data, indent=4)\n",
    "                #print(json_object)\n",
    "                with open(file_path, \"w\") as outfile:\n",
    "                    outfile.write(json_object)\n",
    "            if source == \"dailymail\":\n",
    "                os.makedirs(\"crawl/dailymail\", exist_ok=True)\n",
    "                file_path = \"crawl/dailymail/\"+date.replace(\"-\",\"\")+f\"{idx:0>4}\"+\".json\"\n",
    "                data['title'] = \" \".join(bf.h1.text.split())\n",
    "                data['url'] = url\n",
    "                data['date'] = f\"{date[:4]}-{date[4:6]}-{date[6:]}\"\n",
    "                data['keywords'] = keywords_generator(re.sub(r\"[^\\w\\s]+\",\"\",\" \".join(bf.find_all(attrs={\"itemprop\": \"articleBody\"})[0].text.split()).lower()))\n",
    "                category = url.split(\"/\")\n",
    "                data['category'] = [category[3]]\n",
    "                if \"-\" not in category[4]:\n",
    "                   data['category'] = [category[3],category[4]]\n",
    "                json_object = json.dumps(data, indent=4)\n",
    "                #print(json_object)\n",
    "                with open(file_path, \"w\") as outfile:\n",
    "                    outfile.write(json_object)\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred while fetching {url}: {e}')\n",
    "        pass\n",
    "        \n",
    "def keywords_generator(content):\n",
    "    tokens = word_tokenize(content)\n",
    "    tags = pos_tag(tokens) \n",
    "    nouns = [word for (word, tag) in tags if tag == \"NN\"] \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    keywords = [word for word in nouns if word not in stop_words]\n",
    "    top_keywords = [x[0] for x in Counter(keywords).most_common(100)]\n",
    "    return top_keywords\n",
    "\n",
    "def retry(url):\n",
    "    \n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    print(url)\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            res = r.get(url,headers)\n",
    "            return res, False\n",
    "        except r.exceptions.RequestException as e:\n",
    "            print(f'Error occurred while fetching {url}: {e}')\n",
    "            if attempt < 4:\n",
    "                print(f'Retrying in {2} seconds...attempt: {attempt}')\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                print('Max retries exceeded. Giving up.')\n",
    "                return None, True\n",
    "fetch_bbc()\n",
    "fetch_nbc()\n",
    "fetch_cnn()\n",
    "fetch_dailymail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
